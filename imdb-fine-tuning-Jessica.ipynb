{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "\n",
    "* PEFT technique: LoRA\n",
    "* Model: DistilBERT (distilbert-base-uncased)\n",
    "* Evaluation approach: Hugging Face Trainer\n",
    "* Fine-tuning dataset: imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c6ec17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: evaluate in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (0.4.6)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (2.3.3)\n",
      "Requirement already satisfied: dill in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (0.35.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from datasets>=2.0.0->evaluate) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
      "Requirement already satisfied: anyio in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "057c2ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (4.57.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (4.2.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (1.10.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: anyio in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jessica.jumelles\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632c138f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent-Ark/Toucan-1.5M,fka/awesome-chatgpt-prompts,openai/gdpval,Salesforce/Webscale-RL,Jr23xd23/ArabicText-Large,WNJXYK/MATH-Reasoning-Paths,zwhe99/DeepMath-103K,HuggingFaceFW/finepdfs,WNJXYK/MathOdyssey-Reasoning-Paths,WNJXYK/OlympiadBench-Reasoning-Paths,Anthropic/hh-rlhf,google/svq,allenai/WildChat-1M,hotpotqa/hotpot_qa,WNJXYK/AIME_1983_2024-Reasoning-Paths,MTSAIR/MWS-Vision-Bench,WNJXYK/LawQA,omniretarget/OmniRetarget_Dataset,wikimedia/wikipedia,HuggingFaceFW/fineweb\n"
     ]
    }
   ],
   "source": [
    "#How to find datasets on HuggingFace Hub\n",
    "from huggingface_hub import list_datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "datasets_list = list_datasets(limit=20)  # Limit to 20 datasets to avoid rate limit\n",
    "#From the HuggingFace Hub lets grab a dataset - easy one is the imdb dataset    \n",
    "print(','.join(dataset.id for dataset in datasets_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91e3d567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plain_text']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#some datasets have multiple configurations\n",
    "#we can check the available configurations for a dataset\n",
    "from datasets import get_dataset_config_names\n",
    "print(get_dataset_config_names(\"imdb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87e821b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier\\'s plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it\\'s the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...', 'label': 1, 'input_ids': [101, 2045, 2003, 2053, 7189, 2012, 2035, 2090, 3481, 3771, 1998, 6337, 2099, 2021, 1996, 2755, 2008, 2119, 2024, 2610, 2186, 2055, 6355, 6997, 1012, 6337, 2099, 3504, 15594, 2100, 1010, 3481, 3771, 3504, 4438, 1012, 6337, 2099, 14811, 2024, 3243, 3722, 1012, 3481, 3771, 1005, 1055, 5436, 2024, 2521, 2062, 8552, 1012, 1012, 1012, 3481, 3771, 3504, 2062, 2066, 3539, 8343, 1010, 2065, 2057, 2031, 2000, 3962, 12319, 1012, 1012, 1012, 1996, 2364, 2839, 2003, 5410, 1998, 6881, 2080, 1010, 2021, 2031, 1000, 17936, 6767, 7054, 3401, 1000, 1012, 2111, 2066, 2000, 12826, 1010, 2000, 3648, 1010, 2000, 16157, 1012, 2129, 2055, 2074, 9107, 1029, 6057, 2518, 2205, 1010, 2111, 3015, 3481, 3771, 3504, 2137, 2021, 1010, 2006, 1996, 2060, 2192, 1010, 9177, 2027, 9544, 2137, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'text': \"<br /><br />When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?<br /><br />Very quickly, however, I realized that this story was about A Thousand Other Things besides just Acres. I started crying and couldn't stop until long after the movie ended. Thank you Jane, Laura and Jocelyn, for bringing us such a wonderfully subtle and compassionate movie! Thank you cast, for being involved and portraying the characters with such depth and gentleness!<br /><br />I recognized the Angry sister; the Runaway sister and the sister in Denial. I recognized the Abusive Husband and why he was there and then the Father, oh oh the Father... all superbly played. I also recognized myself and this movie was an eye-opener, a relief, a chance to face my OWN truth and finally doing something about it. I truly hope A Thousand Acres has had the same effect on some others out there.<br /><br />Since I didn't understand why the cover said the film was about sisters fighting over land -they weren't fighting each other at all- I watched it a second time. Then I was able to see that if one hadn't lived a similar story, one would easily miss the overwhelming undercurrent of dread and fear and the deep bond between the sisters that runs through it all. That is exactly the reason why people in general often overlook the truth about their neighbors for instance.<br /><br />But yet another reason why this movie is so perfect!<br /><br />I don't give a rat's ass (pardon my French) about to what extend the King Lear story is followed. All I know is that I can honestly say: this movie has changed my life.<br /><br />Keep up the good work guys, you CAN and DO make a difference.<br /><br />\", 'label': 1, 'input_ids': [101, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2043, 1045, 4895, 13203, 5051, 10985, 2135, 12524, 1037, 4595, 4631, 1010, 1045, 2245, 1045, 2001, 1999, 2005, 2019, 14036, 2332, 26511, 2466, 1998, 1997, 2607, 9393, 1052, 7959, 13355, 2121, 2001, 1999, 2009, 1010, 2061, 2054, 2071, 2175, 3308, 1029, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2200, 2855, 1010, 2174, 1010, 1045, 3651, 2008, 2023, 2466, 2001, 2055, 1037, 4595, 2060, 2477, 4661, 2074, 4631, 1012, 1045, 2318, 6933, 1998, 2481, 1005, 1056, 2644, 2127, 2146, 2044, 1996, 3185, 3092, 1012, 4067, 2017, 4869, 1010, 6874, 1998, 23786, 1010, 2005, 5026, 2149, 2107, 1037, 6919, 2135, 11259, 1998, 29353, 3185, 999, 4067, 2017, 3459, 1010, 2005, 2108, 2920, 1998, 17274, 1996, 3494, 2007, 2107, 5995, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import evaluate \n",
    "\n",
    "# --- Load Tokenizer and Dataset ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = small_test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "##Check the we tokenized the examples correctly\n",
    "print(tokenized_train[0])\n",
    "print(tokenized_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2ba6217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Model ---\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    ")\n",
    "\n",
    "# Freeze base model parameters to train only classifier head\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "    model.classifier \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c22c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59db7c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessica.jumelles\\AppData\\Local\\Temp\\ipykernel_17268\\438697938.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model (classification head only)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessica.jumelles\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 01:47, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessica.jumelles\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessica.jumelles\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.6804912686347961, 'eval_accuracy': 0.61, 'eval_runtime': 37.1188, 'eval_samples_per_second': 26.941, 'eval_steps_per_second': 1.697, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# --- Metric function ---\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (preds == labels).mean()}\n",
    "\n",
    "# --- Data collator ---\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# --- Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# --- Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "# --- Train and Evaluate ---\n",
    "print(\"Training the model (classification head only)...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2f5d3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 887,042 || all params: 67,842,052 || trainable%: 1.3075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessica.jumelles\\AppData\\Local\\Temp\\ipykernel_17268\\2147321303.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  lora_trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning the model with LoRA adapters...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 06:06, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.438738</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.419289</td>\n",
       "      <td>0.823000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jessica.jumelles\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=126, training_loss=0.5073725079733228, metrics={'train_runtime': 368.6446, 'train_samples_per_second': 5.425, 'train_steps_per_second': 0.342, 'total_flos': 67596195840000.0, 'train_loss': 0.5073725079733228, 'epoch': 2.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# APPLY PEFT (LoRA) FOR LIGHTWEIGHT FINE-TUNING\n",
    "# ============================================================\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    ")\n",
    "\n",
    "\n",
    "# --- Create a LoRA configuration ---\n",
    "# Specify target_modules for DistilBERT attention layers so PEFT/LoRA knows where to apply adapters.\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",   # sequence classification\n",
    "    r=8,                   # rank of the LoRA update matrices\n",
    "    lora_alpha=32,         # scaling factor\n",
    "    lora_dropout=0.1,      # dropout for LoRA layers\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],\n",
    ")\n",
    "\n",
    "# --- Wrap the pretrained model with the LoRA adapter ---\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Check trainable parameters (to confirm LoRA is applied)\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "# ============================================================\n",
    "#  TRAIN THE PEFT (LoRA) MODEL\n",
    "# ============================================================\n",
    "\n",
    "# Reuse same training arguments but change output directory\n",
    "lora_training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,   # LoRA can use a slightly higher LR\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Create Trainer for LoRA model\n",
    "lora_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=lora_training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning the model with LoRA adapters...\")\n",
    "lora_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d7805a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA fine-tuned model saved to: ./peft_lora_model\n"
     ]
    }
   ],
   "source": [
    " #SAVE THE PEFT MODEL\n",
    "# ============================================================\n",
    "\n",
    "save_directory = \"./peft_lora_model\"\n",
    "peft_model.save_pretrained(save_directory)\n",
    "print(f\"LoRA fine-tuned model saved to: {save_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e26b2c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\jessica.jumelles\\AppData\\Local\\Temp\\ipykernel_17268\\3205574363.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "C:\\Users\\jessica.jumelles\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the LoRA fine-tuned model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model accuracy: 0.8230\n"
     ]
    }
   ],
   "source": [
    "#LOAD AND EVALUATE THE FINE-TUNED MODEL\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    ")\n",
    "\n",
    "# Load Saved PEFT Model \n",
    "peft_model_loaded = AutoPeftModelForSequenceClassification.from_pretrained(save_directory)\n",
    "\n",
    "# Create Trainer for evaluation\n",
    "eval_trainer = Trainer(\n",
    "    model=peft_model_loaded,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Evaluating the LoRA fine-tuned model...\")\n",
    "final_results = eval_trainer.evaluate()\n",
    "print(f\"Fine-tuned model accuracy: {final_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5a2141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model accuracy: 0.6100\n",
      "Fine-tuned model accuracy: 0.8230\n"
     ]
    }
   ],
   "source": [
    "#compare results\n",
    "print(f\"Base model accuracy: {results['eval_accuracy']:.4f}\")\n",
    "print(f\"Fine-tuned model accuracy: {final_results['eval_accuracy']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
